import csv
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import defaultdict
import json

output_file = r"C:\Users\ayesh\Documents\archive\final_processed_data.json"
forward_index_file = r"C:\Users\ayesh\Documents\archive\forward_index.json"
hitlist_file = r"C:\Users\ayesh\Documents\archive\hitlist.json"

row_limit = 45000

try:
    
    with open(forward_index_file, 'r', encoding='utf-8') as f:
        loaded_data = json.load(f)

  
        total_rows = len(loaded_data)
        print(f"\nTotal rows loaded from {forward_index_file}: {total_rows}\n")

        chunk_size = 100
        for i in range(0, total_rows, chunk_size):
            chunk = loaded_data[i:i + chunk_size]
            print(f"Displaying rows {i + 1} to {min(i + chunk_size, total_rows)}:\n")
            for row in chunk:
                print(row)
            print("\n--- End of Chunk ---\n")

except FileNotFoundError:
    print("Error: File not found. Please check the file path.")
except Exception as e:
    print(f"An error occurred: {e}")
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def create_forward_index(processed_data, limit=15):
    forward_index = defaultdict(lambda: defaultdict(list))
    for idx, data in list(processed_data.items())[:limit]:
        for i, word in enumerate(data['title']):
            forward_index[idx][word].append(i)
        for i, word in enumerate(data['text']):
            forward_index[idx][word].append(i)
    filtered_forward_index = {
        docID: {
            word: positions for word, positions in words.items() if len(positions) > 1
        }
        for docID, words in forward_index.items()
    }
    return filtered_forward_index

try:
    with open(output_file, 'r', encoding='utf-8') as f:
        processed_data = json.load(f)

    forward_index = create_forward_index(processed_data, limit=15)

    with open(forward_index_file, 'w', encoding='utf-8') as f:
        json.dump(forward_index, f, ensure_ascii=False, indent=4)

    hitlist = []
    for docID, words in forward_index.items():
        word_info = {}
        for word, positions in words.items():
            if len(positions) > 1:
                word_info[word] = positions

        if word_info:
            hitlist.append({
                'docID': docID,
                'words': word_info
            })

    with open(hitlist_file, 'w', encoding='utf-8') as f:
        json.dump(hitlist, f, ensure_ascii=False, indent=4)

    print(f"Forward index saved to {forward_index_file}")
    print(f"Hitlist saved to {hitlist_file}")

except Exception as e:
    print(f"Error: {e}")
